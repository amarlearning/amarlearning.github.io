<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Blogs on Amar Prakash Pandey - ᕦ(ò_óˇ)ᕤ</title>
    <link>http://localhost:1313/blog/</link>
    <description>Recent content in Blogs on Amar Prakash Pandey - ᕦ(ò_óˇ)ᕤ</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 30 Mar 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:1313/blog/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>4TB RAM, Yet an OOM Error? Debugging a Spark Memory Mystery</title>
      <link>http://localhost:1313/blog/4tb-ram-yet-an-oom-error-debugging-a-spark-memory-mystery/</link>
      <pubDate>Sun, 30 Mar 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/blog/4tb-ram-yet-an-oom-error-debugging-a-spark-memory-mystery/</guid>
      <description>Introduction Everything seemed right—ample resources, a well-sized cluster, and yet, the Spark job kept failing with an out-of-memory error. Logs pointed to memory allocation failures, but with a 63-node cluster, each equipped with 64GB RAM, this shouldn’t have been an issue. We tweaked configurations, analyzed logs, and even considered scaling up the cluster. But the real solution? It wasn’t what we expected.&#xA;The Data Challenge Our task involved processing three massive datasets:</description>
    </item>
    <item>
      <title>Deep Dive into Spark Jobs and Stages</title>
      <link>http://localhost:1313/blog/deep-dive-into-spark-jobs-and-stages/</link>
      <pubDate>Sun, 23 Mar 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/blog/deep-dive-into-spark-jobs-and-stages/</guid>
      <description>When working with large-scale data processing using Apache Spark, understanding how jobs and stages work is crucial to optimizing performance. This blog is for those who already have some experience with Spark and want to dig deeper into the internal mechanics of jobs and stages.&#xA;Spark Transformations and Actions In Spark, operations are classified into two main categories: Transformations and Actions.&#xA;Transformations: These operations do not trigger execution but define a new dataset from an existing one.</description>
    </item>
    <item>
      <title>Balancing the RUM Conjecture: Navigating Database Trade-Offs</title>
      <link>http://localhost:1313/blog/balancing-the-rum-conjecture-navigating-database-trade-offs/</link>
      <pubDate>Tue, 05 Nov 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/blog/balancing-the-rum-conjecture-navigating-database-trade-offs/</guid>
      <description>When designing databases, there&amp;rsquo;s a constant balancing act among three main factors:&#xA;Read times Update cost Memory/storage overhead The RUM Conjecture suggests that optimizing any two of these factors will negatively impact the third. Essentially, you can only choose two out of the three to prioritize in any design.&#xA;Example: Log-Structured Databases Consider a log-structured database:&#xA;Update-optimized: Records are appended at the end of the file, allowing efficient updates. Low memory/storage overhead: There’s no additional indexing, saving on storage.</description>
    </item>
    <item>
      <title>The CAP Theorem: Balancing the Big Three in Distributed Databases</title>
      <link>http://localhost:1313/blog/the-cap-theorem-balancing-the-big-three-in-distributed-databases/</link>
      <pubDate>Tue, 15 Oct 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/blog/the-cap-theorem-balancing-the-big-three-in-distributed-databases/</guid>
      <description>The CAP theorem, also known as Brewer’s theorem (named after computer scientist Eric Brewer), defines a fundamental trade-off in distributed systems: any distributed data store can provide only two out of three guarantees at any time:&#xA;C: Consistency A: Availability P: Partition Tolerance What Do These Terms Mean? Consistency (C): Every read receives the most recent write or an error. This means that the data you access is guaranteed to be the latest version, or the system will notify you that something went wrong.</description>
    </item>
    <item>
      <title>Fine-Tuning Shuffle Partitions in Apache Spark for Maximum Efficiency</title>
      <link>http://localhost:1313/blog/fine-tuning-shuffle-partitions-in-apache-spark-for-maximum-efficiency/</link>
      <pubDate>Fri, 24 May 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/blog/fine-tuning-shuffle-partitions-in-apache-spark-for-maximum-efficiency/</guid>
      <description>Apache Spark&amp;rsquo;s shuffle partitions play a critical role in data processing, especially during operations like joins and aggregations. Properly configuring these partitions is essential for optimizing performance.&#xA;Default Shuffle Partition Count By default, Spark sets the shuffle partition count to 200. While this may work for small datasets (less than 20 GB), it is usually inadequate for larger data sizes. Besides, who would work with just 20 GB of data on Spark?</description>
    </item>
    <item>
      <title>Handling Large Broadcast Joins in Apache Spark</title>
      <link>http://localhost:1313/blog/handling-large-broadcast-joins-in-apache-spark/</link>
      <pubDate>Wed, 22 May 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/blog/handling-large-broadcast-joins-in-apache-spark/</guid>
      <description>In Apache Spark, efficient data processing often relies on the use of broadcast joins. However, when the dataset exceeds a certain size, specifically 8GB, you may encounter the following error:&#xA;Caused by: org.apache.spark.SparkException: Cannot broadcast the table that is larger than 8GB: 13 GB This error arises because Spark is attempting to broadcast a dataset that is larger than the maximum threshold allowed for broadcast joins. By default, Spark&amp;rsquo;s threshold for broadcasting is set to 8GB.</description>
    </item>
    <item>
      <title>Symptoms of Bad Code</title>
      <link>http://localhost:1313/blog/symptoms-of-bad-code/</link>
      <pubDate>Tue, 19 Jul 2022 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/blog/symptoms-of-bad-code/</guid>
      <description>1. Rigidity Rigidity is the tendency of the system to be hard to change. Code that has dependencies that snake out in so many directions and you cannot make an isolated change without changing everything around it. Rigidity causes compile time error. 2. Fragility A system is fragile when a small change in one module causes other unrelated modules to misbehave.&#xA;It is the tendency of the code to break in many places even when you make changes in one place.</description>
    </item>
    <item>
      <title>Docker - the right way</title>
      <link>http://localhost:1313/blog/docker---the-right-way/</link>
      <pubDate>Sun, 23 Jan 2022 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/blog/docker---the-right-way/</guid>
      <description>Docker is a software framework for building, running, and managing containers on servers and the cloud. Here are the several best practices for using Docker in production to improve security, optimize image size and write cleaner and more maintainable Dockerfiles.&#xA;1. Use Official Docker Image as Base Image Always use the official or verified base image when writing the docker file. Let&amp;rsquo;s say you are developing a java application and want to build it and run it as a docker image.</description>
    </item>
    <item>
      <title>GitOps - the easy way</title>
      <link>http://localhost:1313/blog/gitops---the-easy-way/</link>
      <pubDate>Tue, 04 Jan 2022 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/blog/gitops---the-easy-way/</guid>
      <description>What is GitOps? Treat the infrastructure as code the same way as application code.&#xA;Separate repository for Infrastructure as code. DevOps pipeline. How does GitOps works? Infrastructure as Code hosted on Git repository.&#xA;Version controlled. Team collaboration. Use branching strategy to merge code in git repository. With CI pipeline to test the code. With CD pipeline to apply the changes to the Infrastructure. With the above steps we achieve: Automated Process.</description>
    </item>
    <item>
      <title>Finger Detection and Tracking using OpenCV and Python</title>
      <link>http://localhost:1313/blog/finger-detection-and-tracking-using-opencv-and-python/</link>
      <pubDate>Sat, 28 Jul 2018 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/blog/finger-detection-and-tracking-using-opencv-and-python/</guid>
      <description>TL;DR. Code is here.&#xA;Finger detection is an important feature of many computer vision applications. In this application, A histogram based approach is used to separate out the hand from the background frame. Thresholding and Filtering techniques are used for background cancellation to obtain optimum results.&#xA;One of the challenges that I faced in detecting fingers is differentiating a hand from the background and identifying the tip of a finger.</description>
    </item>
    <item>
      <title>What is Google Summer of Code? How to prepare for it?</title>
      <link>http://localhost:1313/blog/what-is-google-summer-of-code---how-to-prepare-for-it/</link>
      <pubDate>Sun, 02 Jul 2017 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/blog/what-is-google-summer-of-code---how-to-prepare-for-it/</guid>
      <description>We will talk about Google Summer of Code but before that let’s talk about what Open Source Development is. Yes, it’s very important.&#xA;What is open source development? Open-source software development is the process by which open-source software, or similar software whose source code is publicly available, is developed. These are software products available with its source code under an open-source license to study, change, and improve its design.</description>
    </item>
  </channel>
</rss>
