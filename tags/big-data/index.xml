<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Big Data on Amar Prakash Pandey - ᕦ(ò_óˇ)ᕤ</title>
    <link>https://amarpandey.me/tags/big-data/</link>
    <description>Recent content in Big Data on Amar Prakash Pandey - ᕦ(ò_óˇ)ᕤ</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 07 Apr 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://amarpandey.me/tags/big-data/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>From Bottlenecks to Balance: Dynamic Skew Join Fixes in Spark</title>
      <link>https://amarpandey.me/blog/from-bottlenecks-to-balance-dynamic-skew-join-fixes-in-spark/</link>
      <pubDate>Mon, 07 Apr 2025 00:00:00 +0000</pubDate>
      <guid>https://amarpandey.me/blog/from-bottlenecks-to-balance-dynamic-skew-join-fixes-in-spark/</guid>
      <description>When working with large datasets in Spark, joins are a common operation. But what happens when data distribution isn’t uniform? Let’s dive into a real-world scenario to understand why dynamic skew join optimization is not just useful, but often essential.&#xA;The Problem Setup Assume we have two large tables and we&amp;rsquo;re trying to join them using the following Spark SQL:&#xA;SELECT * FROM large_table_one JOIN large_table_two ON large_table_one.key = large_table_two.</description>
    </item>
    <item>
      <title>4TB RAM, Yet an OOM Error? Debugging a Spark Memory Mystery</title>
      <link>https://amarpandey.me/blog/4tb-ram-yet-an-oom-error-debugging-a-spark-memory-mystery/</link>
      <pubDate>Sun, 30 Mar 2025 00:00:00 +0000</pubDate>
      <guid>https://amarpandey.me/blog/4tb-ram-yet-an-oom-error-debugging-a-spark-memory-mystery/</guid>
      <description>Everything seemed right—ample resources, a well-sized cluster, and yet, the Spark job kept failing with an out-of-memory error. Logs pointed to memory allocation failures, but with a 63-node cluster, each equipped with 64GB RAM, this shouldn’t have been an issue. We tweaked configurations, analyzed logs, and even considered scaling up the cluster. But the real solution? It wasn’t what we expected.&#xA;The Data Challenge Our task involved processing three datasets:</description>
    </item>
    <item>
      <title>Deep Dive into Spark Jobs and Stages</title>
      <link>https://amarpandey.me/blog/deep-dive-into-spark-jobs-and-stages/</link>
      <pubDate>Sun, 23 Mar 2025 00:00:00 +0000</pubDate>
      <guid>https://amarpandey.me/blog/deep-dive-into-spark-jobs-and-stages/</guid>
      <description>When working with large-scale data processing using Apache Spark, understanding how jobs and stages work is crucial to optimizing performance. This blog is for those who already have some experience with Spark and want to dig deeper into the internal mechanics of jobs and stages.&#xA;Spark Transformations and Actions In Spark, operations are classified into two main categories: Transformations and Actions.&#xA;Transformations: These operations do not trigger execution but define a new dataset from an existing one.</description>
    </item>
  </channel>
</rss>
