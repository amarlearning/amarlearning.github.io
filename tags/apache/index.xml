<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Apache on Amar Prakash Pandey - ᕦ(ò_óˇ)ᕤ</title>
    <link>https://amarpandey.me/tags/apache/</link>
    <description>Recent content in Apache on Amar Prakash Pandey - ᕦ(ò_óˇ)ᕤ</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 24 May 2024 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://amarpandey.me/tags/apache/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Fine-Tuning Shuffle Partitions in Apache Spark for Maximum Efficiency</title>
      <link>https://amarpandey.me/blog/optimizing-shuffle-partitions-in-apache-spark/</link>
      <pubDate>Fri, 24 May 2024 00:00:00 +0000</pubDate>
      <guid>https://amarpandey.me/blog/optimizing-shuffle-partitions-in-apache-spark/</guid>
      <description>Apache Spark&amp;rsquo;s shuffle partitions play a critical role in data processing, especially during operations like joins and aggregations. Properly configuring these partitions is essential for optimizing performance.&#xA;Default Shuffle Partition Count By default, Spark sets the shuffle partition count to 200. While this may work for small datasets (less than 20 GB), it is usually inadequate for larger data sizes. Besides, who would work with just 20 GB of data on Spark?</description>
    </item>
    <item>
      <title>Handling Large Broadcast Joins in Apache Spark</title>
      <link>https://amarpandey.me/blog/handling-large-broadcast-joins-in-apache-spark/</link>
      <pubDate>Wed, 22 May 2024 00:00:00 +0000</pubDate>
      <guid>https://amarpandey.me/blog/handling-large-broadcast-joins-in-apache-spark/</guid>
      <description>In Apache Spark, efficient data processing often relies on the use of broadcast joins. However, when the dataset exceeds a certain size, specifically 8GB, you may encounter the following error:&#xA;Caused by: org.apache.spark.SparkException: Cannot broadcast the table that is larger than 8GB: 13 GB This error arises because Spark is attempting to broadcast a dataset that is larger than the maximum threshold allowed for broadcast joins. By default, Spark&amp;rsquo;s threshold for broadcasting is set to 8GB.</description>
    </item>
  </channel>
</rss>
